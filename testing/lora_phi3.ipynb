{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a63f68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3VForCausalLM(\n",
      "  (model): Phi3VModel(\n",
      "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (vision_embed_tokens): Phi3ImageEmbedding(\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (wte): Embedding(32064, 3072, padding_idx=32000)\n",
      "      (img_processor): CLIPVisionModel(\n",
      "        (vision_model): CLIPVisionTransformer(\n",
      "          (embeddings): CLIPVisionEmbeddings(\n",
      "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "            (position_embedding): Embedding(577, 1024)\n",
      "          )\n",
      "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder): CLIPEncoder(\n",
      "            (layers): ModuleList(\n",
      "              (0-23): 24 x CLIPEncoderLayer(\n",
      "                (self_attn): CLIPSdpaAttention(\n",
      "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                )\n",
      "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): CLIPMLP(\n",
      "                  (activation_fn): QuickGELUActivation()\n",
      "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (img_projection): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x Phi3DecoderLayer(\n",
      "        (self_attn): Phi3Attention(\n",
      "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
      "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
      "          (rotary_emb): Phi3SuScaledRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Phi3MLP(\n",
      "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
      "          (activation_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Phi3RMSNorm()\n",
      "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_attention_layernorm): Phi3RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Phi3RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM \n",
    "from transformers import AutoProcessor\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForMaskedLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "import math\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='eager')\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"user\", \"content\": \"\"\" <|image_1|> Your task is to extract the information for the fields provided below. Extract the information in JSON format according to the following JSON schema:{\n",
    "  \"$defs\": {\n",
    "    \"InvoiceLineItem\": {\n",
    "      \"properties\": {\n",
    "        \"name\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The name of the menu item\",\n",
    "          \"title\": \"Name\"\n",
    "        },\n",
    "        \"net_unit_price\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The unit price before tax\",\n",
    "          \"title\": \"Net Unit Price\"\n",
    "        },\n",
    "        \"unit_tax\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Tax amount per unit\",\n",
    "          \"title\": \"Unit Tax\"\n",
    "        },\n",
    "        \"gross_unit_price\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Unit price including tax\",\n",
    "          \"title\": \"Gross Unit Price\"\n",
    "        },\n",
    "        \"quantity\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Quantity ordered (can be decimal for weights/volumes/litres)\",\n",
    "          \"title\": \"Quantity\"\n",
    "        },\n",
    "        \"net_price\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Total price before tax (quantity \\u00d7 net_unit_price)\",\n",
    "          \"title\": \"Net Price\"\n",
    "        },\n",
    "        \"tax_amount\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Total tax amount (quantity \\u00d7 unit_tax)\",\n",
    "          \"title\": \"Tax Amount\"\n",
    "        },\n",
    "        \"gross_price\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Total price including tax\",\n",
    "          \"title\": \"Gross Price\"\n",
    "        },\n",
    "        \"sub_items\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"items\": {\n",
    "                \"$ref\": \"#/$defs/InvoiceSubLineItem\"\n",
    "              },\n",
    "              \"type\": \"array\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Additional components or modifications\",\n",
    "          \"identifier_field_name\": \"nm\",\n",
    "          \"title\": \"Sub Items\"\n",
    "        },\n",
    "        \"net_sub_items_total\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Total price of all sub-items before tax\",\n",
    "          \"title\": \"Net Sub Items Total\"\n",
    "        },\n",
    "        \"gross_sub_items_total\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Total price of all sub-items including tax\",\n",
    "          \"title\": \"Gross Sub Items Total\"\n",
    "        },\n",
    "        \"net_total\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Combined net price of item and sub-items before discounts\",\n",
    "          \"title\": \"Net Total\"\n",
    "        },\n",
    "        \"net_discounts\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"items\": {\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"type\": \"array\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Discounts applied to net total of this item\",\n",
    "          \"title\": \"Net Discounts\",\n",
    "          \"unordered\": true\n",
    "        },\n",
    "        \"total_tax\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Combined tax amount for item and sub-items\",\n",
    "          \"title\": \"Total Tax\"\n",
    "        },\n",
    "        \"gross_discounts\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"items\": {\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"type\": \"array\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Discounts applied to the gross total of this item\",\n",
    "          \"title\": \"Gross Discounts\",\n",
    "          \"unordered\": true\n",
    "        },\n",
    "        \"gross_total\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"Final price including tax and after discounts\",\n",
    "          \"title\": \"Gross Total\"\n",
    "        }\n",
    "      },\n",
    "      \"title\": \"InvoiceLineItem\",\n",
    "      \"type\": \"object\"\n",
    "    },\n",
    "    \"InvoiceSubLineItem\": {\n",
    "      \"properties\": {\n",
    "        \"name\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The name of the sub-item or modification\",\n",
    "          \"title\": \"Name\"\n",
    "        },\n",
    "        \"net_unit_price\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The unit price of the sub-item before tax\",\n",
    "          \"title\": \"Net Unit Price\"\n",
    "        },\n",
    "        \"unit_tax\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The tax amount per unit of the sub-item\",\n",
    "          \"title\": \"Unit Tax\"\n",
    "        },\n",
    "        \"gross_unit_price\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The unit price of the sub-item including tax\",\n",
    "          \"title\": \"Gross Unit Price\"\n",
    "        },\n",
    "        \"quantity\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The quantity of the sub-item (can be a decimal for items sold by weight or volume)\",\n",
    "          \"title\": \"Quantity\"\n",
    "        },\n",
    "        \"net_price\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The total price of the sub-item before tax\",\n",
    "          \"title\": \"Net Price\"\n",
    "        },\n",
    "        \"tax_amount\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The total tax amount for the sub-item\",\n",
    "          \"title\": \"Tax Amount\"\n",
    "        },\n",
    "        \"gross_price\": {\n",
    "          \"anyOf\": [\n",
    "            {\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"null\"\n",
    "            }\n",
    "          ],\n",
    "          \"default\": null,\n",
    "          \"description\": \"The total price of the sub-item including tax\",\n",
    "          \"title\": \"Gross Price\"\n",
    "        }\n",
    "      },\n",
    "      \"title\": \"InvoiceSubLineItem\",\n",
    "      \"type\": \"object\"\n",
    "    }\n",
    "  },\n",
    "  \"properties\": {\n",
    "    \"base_taxable_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The base amount that is subject to tax\",\n",
    "      \"title\": \"Base Taxable Amount\"\n",
    "    },\n",
    "    \"net_discounts\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"items\": {\n",
    "            \"type\": \"string\"\n",
    "          },\n",
    "          \"type\": \"array\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Discounts applied to taxable amount before tax calculation\",\n",
    "      \"title\": \"Net Discounts\",\n",
    "      \"unordered\": true\n",
    "    },\n",
    "    \"net_service_charge\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Service charge applied to taxable amount before tax calculation\",\n",
    "      \"title\": \"Net Service Charge\"\n",
    "    },\n",
    "    \"taxable_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The amount that is subject to tax. This is the base amount plus net discounts and net service charges\",\n",
    "      \"title\": \"Taxable Amount\"\n",
    "    },\n",
    "    \"non_taxable_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The base amount that is not subject to tax\",\n",
    "      \"title\": \"Non Taxable Amount\"\n",
    "    },\n",
    "    \"net_total\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Sum of taxable and non-taxable amounts with their modifiers\",\n",
    "      \"title\": \"Net Total\"\n",
    "    },\n",
    "    \"tax_rate\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Tax rate percentage applied to taxable amount\",\n",
    "      \"title\": \"Tax Rate\"\n",
    "    },\n",
    "    \"tax_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Total amount of tax on the invoice\",\n",
    "      \"title\": \"Tax Amount\"\n",
    "    },\n",
    "    \"base_gross_total\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The base amount that is subject to gross discounts and service charges\",\n",
    "      \"title\": \"Base Gross Total\"\n",
    "    },\n",
    "    \"gross_discounts\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"items\": {\n",
    "            \"type\": \"string\"\n",
    "          },\n",
    "          \"type\": \"array\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Discounts applied to entire net total after tax\",\n",
    "      \"title\": \"Gross Discounts\",\n",
    "      \"unordered\": true\n",
    "    },\n",
    "    \"gross_service_charge\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Service charge applied to entire net total after tax\",\n",
    "      \"title\": \"Gross Service Charge\"\n",
    "    },\n",
    "    \"gross_total\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Final amount after all taxes and modifications\",\n",
    "      \"title\": \"Gross Total\"\n",
    "    },\n",
    "    \"rounding_adjustment\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Amount added/subtracted to round to desired precision\",\n",
    "      \"title\": \"Rounding Adjustment\"\n",
    "    },\n",
    "    \"commission_fee\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Commission amount deducted from total\",\n",
    "      \"title\": \"Commission Fee\"\n",
    "    },\n",
    "    \"due_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The amount due for the transaction before considering prior balance\",\n",
    "      \"title\": \"Due Amount\"\n",
    "    },\n",
    "    \"prior_balance\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Previous balance or credit applied to the current transaction\",\n",
    "      \"title\": \"Prior Balance\"\n",
    "    },\n",
    "    \"net_due_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The final amount due after applying prior balance\",\n",
    "      \"title\": \"Net Due Amount\"\n",
    "    },\n",
    "    \"paid_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The total amount paid by the customer\",\n",
    "      \"title\": \"Paid Amount\"\n",
    "    },\n",
    "    \"change_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The amount returned to the customer if overpayment occurred\",\n",
    "      \"title\": \"Change Amount\"\n",
    "    },\n",
    "    \"cash_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The amount paid in cash\",\n",
    "      \"title\": \"Cash Amount\"\n",
    "    },\n",
    "    \"creditcard_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The amount paid by credit card\",\n",
    "      \"title\": \"Creditcard Amount\"\n",
    "    },\n",
    "    \"emoney_amount\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The amount paid using electronic money\",\n",
    "      \"title\": \"Emoney Amount\"\n",
    "    },\n",
    "    \"other_payments\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"items\": {\n",
    "            \"type\": \"string\"\n",
    "          },\n",
    "          \"type\": \"array\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Amounts paid using other methods (e.g., coupons, vouchers)\",\n",
    "      \"title\": \"Other Payments\",\n",
    "      \"unordered\": true\n",
    "    },\n",
    "    \"menutype_count\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The number of distinct menu item types in the order\",\n",
    "      \"title\": \"Menutype Count\"\n",
    "    },\n",
    "    \"menuquantity_sum\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"The total quantity of all menu items ordered\",\n",
    "      \"title\": \"Menuquantity Sum\"\n",
    "    },\n",
    "    \"line_items\": {\n",
    "      \"anyOf\": [\n",
    "        {\n",
    "          \"items\": {\n",
    "            \"$ref\": \"#/$defs/InvoiceLineItem\"\n",
    "          },\n",
    "          \"type\": \"array\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"null\"\n",
    "        }\n",
    "      ],\n",
    "      \"default\": null,\n",
    "      \"description\": \"Detailed list of individual items in the order\",\n",
    "      \"identifier_field_name\": \"nm\",\n",
    "      \"title\": \"Line Items\"\n",
    "    }\n",
    "  },\n",
    "  \"title\": \"Invoice\",\n",
    "  \"type\": \"object\"\n",
    "}\"\"\"}\n",
    "]\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db0a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    json_objects = []\n",
    "    for line in lines:\n",
    "        obj = json.loads(line.strip())\n",
    "        json_objects.append(obj)\n",
    "    \n",
    "    target = []\n",
    "    image = []\n",
    "    for label in json_objects:    \n",
    "        target.append(label[\"target\"])\n",
    "        image.append(label[\"id\"])\n",
    "    \n",
    "    return image, target\n",
    "\n",
    "label_path = r'C:\\Users\\Admin\\Documents\\Internship\\sroie\\train-documents.jsonl'\n",
    "label_val = r'C:\\Users\\Admin\\Documents\\Internship\\sroie\\validation-documents.jsonl'\n",
    "label_test = r'C:\\Users\\Admin\\Documents\\Internship\\sroie\\test-documents.jsonl'\n",
    "image, labels = load_json_lines(label_path)\n",
    "image_val, labels_val = load_json_lines(label_val)\n",
    "image_test, labels_test = load_json_lines(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59551a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'label'],\n",
      "    num_rows: 499\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_data(images, target, path):\n",
    "    images_files = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    for image in images:\n",
    "        image_with_extension = f\"{image}.jpg\"\n",
    "        file_path = os.path.join(path, image_with_extension)\n",
    "        img = Image.open(file_path)\n",
    "        images_files.append(img)\n",
    "        string_data = json.dumps(target[i])\n",
    "        labels.append(string_data)\n",
    "        i+=1  \n",
    "                \n",
    "    return {'image': images_files, 'label': labels}\n",
    "\n",
    "data_path = r\"C:\\Users\\Admin\\Documents\\Internship\\sroie\\images\"\n",
    "\n",
    "\n",
    "data_dict = load_data(image, labels, data_path)\n",
    "dict_val = load_data(image_val,labels_val, data_path)\n",
    "dict_test = load_data(image_test,labels_test,data_path)\n",
    "\n",
    "\n",
    "data_train = Dataset.from_dict(data_dict)\n",
    "data_val = Dataset.from_dict(dict_val)\n",
    "data_test = Dataset.from_dict(dict_val)\n",
    "\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2833d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_dataset(train,val,test):\n",
    "    dataset = DatasetDict({\n",
    "        'train': train,\n",
    "        'validation': val,\n",
    "        'test': test\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "dataset = get_full_dataset(data_train,data_val,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b90a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 499/499 [03:30<00:00,  2.37 examples/s]\n",
      "Map: 100%|██████████| 124/124 [00:37<00:00,  3.27 examples/s]\n",
      "Map: 100%|██████████| 124/124 [00:35<00:00,  3.52 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'pixel_values', 'image_sizes', 'labels'],\n",
      "        num_rows: 499\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'pixel_values', 'image_sizes', 'labels'],\n",
      "        num_rows: 124\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'pixel_values', 'image_sizes', 'labels'],\n",
      "        num_rows: 124\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    images = example['image']  \n",
    "    labels = example['label']  \n",
    "    prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    if isinstance(labels[0], str): \n",
    "        label_inputs = processor.tokenizer(\n",
    "            labels,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        labels = label_inputs[\"input_ids\"] \n",
    "    inputs[\"labels\"] = labels\n",
    "\n",
    "    return inputs\n",
    "tokenized_dataset = dataset.map(tokenize_function,batched=True,num_proc=1, batch_size=1,remove_columns=[\"image\", \"label\"], writer_batch_size=100, load_from_cache_file=False,)\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458291da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=128, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1,  \n",
    "    target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\"], \n",
    "    bias = 'none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a168692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25,165,824 || all params: 4,171,787,264 || trainable%: 0.6032\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637526d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlambd-22bi13234\u001b[0m (\u001b[33mlambd-22bi13234-university-of-science-and-technology-of-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Admin\\Documents\\Internship\\Internship_larochelle\\testing\\wandb\\run-20250408_100734-kjw8gz6t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lambd-22bi13234-university-of-science-and-technology-of-/huggingface/runs/kjw8gz6t' target=\"_blank\">testing</a></strong> to <a href='https://wandb.ai/lambd-22bi13234-university-of-science-and-technology-of-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lambd-22bi13234-university-of-science-and-technology-of-/huggingface' target=\"_blank\">https://wandb.ai/lambd-22bi13234-university-of-science-and-technology-of-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lambd-22bi13234-university-of-science-and-technology-of-/huggingface/runs/kjw8gz6t' target=\"_blank\">https://wandb.ai/lambd-22bi13234-university-of-science-and-technology-of-/huggingface/runs/kjw8gz6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/372 [00:00<?, ?it/s]C:\\Users\\Admin\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-vision-128k-instruct\\c45209e90a4c4f7d16b2e9d48503c7f3e83623ed\\image_embedding_phi3_v.py:197: UserWarning: Phi-3-V modifies `input_ids` in-place and the tokens indicating images will be removed after model forward. If your workflow requires multiple forward passes on the same `input_ids`, please make a copy of `input_ids` before passing it to the model.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 224.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m\n\u001b[0;32m     23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     24\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     25\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     26\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],       \u001b[38;5;66;03m# Pre-tokenized training dataset\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m],   \u001b[38;5;66;03m# Pre-tokenized validation dataset\u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\peft\\peft_model.py:812\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    811\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-vision-128k-instruct\\c45209e90a4c4f7d16b2e9d48503c7f3e83623ed\\modeling_phi3_v.py:1301\u001b[0m, in \u001b[0;36mPhi3VForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, image_sizes, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1298\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1301\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1316\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-vision-128k-instruct\\c45209e90a4c4f7d16b2e9d48503c7f3e83623ed\\modeling_phi3_v.py:1129\u001b[0m, in \u001b[0;36mPhi3VModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, image_sizes, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m image_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_embed_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVision embedding layer is not defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1129\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_embed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-vision-128k-instruct\\c45209e90a4c4f7d16b2e9d48503c7f3e83623ed\\image_embedding_phi3_v.py:210\u001b[0m, in \u001b[0;36mPhi3ImageEmbedding.forward\u001b[1;34m(self, input_ids, pixel_values, image_sizes)\u001b[0m\n\u001b[0;32m    208\u001b[0m num_images, num_crops, c, h, w \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m c \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m h \u001b[38;5;241m==\u001b[39m w \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m336\u001b[39m\n\u001b[1;32m--> 210\u001b[0m img_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_img_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m    211\u001b[0m     num_images, num_crops, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dim_out\n\u001b[0;32m    212\u001b[0m )\n\u001b[0;32m    213\u001b[0m image_features_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhd_feature_transform(img_features, image_sizes)\n\u001b[0;32m    214\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mindex_put(\n\u001b[0;32m    215\u001b[0m     positions, image_features_proj, accumulate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    216\u001b[0m )\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\microsoft\\Phi-3-vision-128k-instruct\\c45209e90a4c4f7d16b2e9d48503c7f3e83623ed\\image_embedding_phi3_v.py:177\u001b[0m, in \u001b[0;36mPhi3ImageEmbedding.get_img_features\u001b[1;34m(self, img_embeds)\u001b[0m\n\u001b[0;32m    174\u001b[0m LAYER_IDX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx\n\u001b[0;32m    175\u001b[0m TYPE_FEATURE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_feature\n\u001b[1;32m--> 177\u001b[0m img_processor_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m img_feature \u001b[38;5;241m=\u001b[39m img_processor_output\u001b[38;5;241m.\u001b[39mhidden_states[LAYER_IDX]\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_FEATURE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1171\u001b[0m, in \u001b[0;36mCLIPVisionModel.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m   1149\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled CLS states\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1097\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m   1094\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[0;32m   1095\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[1;32m-> 1097\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1105\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:877\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[1;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    869\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    870\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    871\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    874\u001b[0m         output_attentions,\n\u001b[0;32m    875\u001b[0m     )\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:618\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    616\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(hidden_states)\n\u001b[1;32m--> 618\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    621\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:574\u001b[0m, in \u001b[0;36mCLIPMLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    573\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states)\n\u001b[1;32m--> 574\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\activations.py:96\u001b[0m, in \u001b[0;36mQuickGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;241;43m1.702\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 224.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "model = model.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              \n",
    "    per_device_train_batch_size=1,      \n",
    "    gradient_accumulation_steps=4,      \n",
    "    num_train_epochs=3,                 \n",
    "    learning_rate=3e-5,                 \n",
    "    weight_decay=0.01,                  \n",
    "    logging_dir=\"./logs\",               \n",
    "    logging_steps=10,                   \n",
    "    save_strategy=\"epoch\",              \n",
    "    evaluation_strategy=\"epoch\",        \n",
    "    push_to_hub=False,                  \n",
    "    report_to=\"wandb\",                  \n",
    "    run_name=\"testing\"    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],       # Pre-tokenized training dataset\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],   # Pre-tokenized validation dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
